"""
Direct Tavily API client without tiktoken dependency
Replaces tavily-python package to avoid Rust compiler issues on Vercel
"""

import requests
import os
import logging
from typing import Dict, List, Optional
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)

class TavilyDirect:
    """Direct Tavily API client for web scraping without dependencies"""
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.getenv('TAVILY_API_KEY')
        self.base_url = "https://api.tavily.com"
        
        if not self.api_key:
            logger.warning("Tavily API key not found - web scraping will use fallback")
    
    def search(self, query: str, max_results: int = 5, search_depth: str = "basic") -> Dict:
        """
        Search using Tavily API directly (no tiktoken dependency)
        """
        try:
            if not self.api_key:
                logger.warning("No Tavily API key - using fallback")
                return self._create_fallback_results(query)
            
            url = f"{self.base_url}/search"
            
            payload = {
                "api_key": self.api_key,
                "query": query,
                "max_results": max_results,
                "search_depth": search_depth,
                "include_answer": True,
                "include_raw_content": True
            }
            
            headers = {
                "Content-Type": "application/json",
                "User-Agent": "Mozilla/5.0 (compatible; TavilyBot/1.0)"
            }
            
            logger.info(f"ðŸ” Searching Tavily for: {query}")
            
            response = requests.post(url, json=payload, headers=headers, timeout=15)
            
            if response.status_code == 200:
                result = response.json()
                logger.info(f"âœ… Tavily search successful: {len(result.get('results', []))} results")
                return result
            elif response.status_code == 401:
                logger.error("âŒ Tavily API key invalid")
                return self._create_fallback_results(query)
            elif response.status_code == 429:
                logger.error("âŒ Tavily rate limit exceeded")
                return self._create_fallback_results(query)
            else:
                logger.error(f"âŒ Tavily API error: {response.status_code}")
                return self._create_fallback_results(query)
            
        except requests.exceptions.Timeout:
            logger.error("âŒ Tavily API timeout")
            return self._create_fallback_results(query)
        except requests.exceptions.ConnectionError:
            logger.error("âŒ Tavily API connection error")
            return self._create_fallback_results(query)
        except Exception as e:
            logger.error(f"âŒ Tavily search error: {str(e)}")
            return self._create_fallback_results(query)
    
    def get_search_context(self, query: str, max_results: int = 3) -> str:
        """
        Get search context formatted for AI processing
        """
        try:
            results = self.search(query, max_results=max_results)
            
            if not results.get('results'):
                return f"No search results found for: {query}"
            
            context = f"Search results for '{query}':\n\n"
            
            # Add answer if available
            if results.get('answer'):
                context += f"Summary: {results['answer']}\n\n"
            
            # Add individual results
            for i, result in enumerate(results['results'], 1):
                context += f"Source {i}:\n"
                context += f"Title: {result.get('title', 'N/A')}\n"
                context += f"URL: {result.get('url', 'N/A')}\n"
                
                # Use content or raw_content
                content = result.get('content') or result.get('raw_content', '')
                if content:
                    context += f"Content: {content[:1000]}...\n\n"
                else:
                    context += f"Content: Not available\n\n"
            
            return context
            
        except Exception as e:
            logger.error(f"âŒ Error getting search context: {str(e)}")
            return f"Error retrieving search results for: {query}"
    
    def scrape_company_info(self, company_name: str, website: str = None) -> Dict:
        """
        Search for company information using Tavily (replaces webscraper)
        """
        try:
            # Create search query
            if website:
                query = f"{company_name} company information {website}"
            else:
                query = f"{company_name} company information official website"
            
            logger.info(f"ðŸ¢ Scraping company info for: {company_name}")
            
            results = self.search(query, max_results=3, search_depth="advanced")
            
            if not results.get('results'):
                return self._create_fallback_company_info(company_name)
            
            # Extract info from search results
            first_result = results['results'][0]
            all_content = ""
            
            # Combine content from all results
            for result in results['results']:
                content = result.get('content', '') or result.get('raw_content', '')
                all_content += content + " "
            
            return {
                'company_name': company_name,
                'website': first_result.get('url', website or ''),
                'description': self._extract_description(all_content, company_name, results.get('answer')),
                'industry': self._extract_industry_from_content(all_content),
                'services': self._extract_services_from_content(all_content),
                'contact_info': self._extract_contact_from_content(all_content),
                'social_media': {},
                'scraped_successfully': True,
                'source': 'tavily_api',
                'search_query': query
            }
            
        except Exception as e:
            logger.error(f"âŒ Company info scraping failed: {str(e)}")
            return self._create_fallback_company_info(company_name)
    
    def _extract_description(self, content: str, company_name: str, answer: str = None) -> str:
        """Extract company description from content"""
        if answer and len(answer) > 50:
            return answer[:500]
        
        if content and len(content) > 100:
            # Find sentences containing company name
            sentences = content.split('.')
            relevant_sentences = []
            
            for sentence in sentences:
                if company_name.lower() in sentence.lower() and len(sentence.strip()) > 50:
                    relevant_sentences.append(sentence.strip())
                    if len(relevant_sentences) >= 2:
                        break
            
            if relevant_sentences:
                description = '. '.join(relevant_sentences)
                return description[:500] + "..." if len(description) > 500 else description
        
        return f"{company_name} is a professional company. More information can be found on their official website."
    
    def _extract_industry_from_content(self, content: str) -> str:
        """Extract industry from content"""
        if not content:
            return "Professional Services"
        
        content_lower = content.lower()
        industry_keywords = {
            'Technology': ['software', 'tech', 'digital', 'IT', 'development', 'programming', 'app', 'platform'],
            'Healthcare': ['health', 'medical', 'hospital', 'clinic', 'pharmacy', 'healthcare'],
            'Finance': ['bank', 'finance', 'investment', 'accounting', 'insurance', 'financial'],
            'Education': ['education', 'school', 'university', 'training', 'learning', 'academic'],
            'Retail': ['retail', 'shop', 'store', 'ecommerce', 'sales', 'marketplace'],
            'Manufacturing': ['manufacturing', 'production', 'factory', 'industrial', 'assembly'],
            'Consulting': ['consulting', 'advisory', 'professional services', 'strategy']
        }
        
        for industry, keywords in industry_keywords.items():
            if any(keyword in content_lower for keyword in keywords):
                return industry
        
        return "Professional Services"
    
    def _extract_services_from_content(self, content: str) -> List[str]:
        """Extract services from content"""
        if not content:
            return ["Professional Services"]
        
        content_lower = content.lower()
        services = []
        
        # Common service keywords
        service_patterns = [
            'services include', 'we offer', 'we provide', 'specializes in', 
            'solutions include', 'products include', 'services are'
        ]
        
        sentences = content.split('.')
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(pattern in sentence_lower for pattern in service_patterns):
                # Extract the part after the service indicator
                for pattern in service_patterns:
                    if pattern in sentence_lower:
                        service_part = sentence_lower.split(pattern)[1] if pattern in sentence_lower else ""
                        if service_part:
                            services.append(sentence.strip()[:100])
                        break
        
        return services[:3] if services else ["Professional Services"]
    
    def _extract_contact_from_content(self, content: str) -> Dict:
        """Extract contact information from content"""
        import re
        
        contact = {}
        
        if not content:
            return contact
        
        try:
            # Extract email
            email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
            emails = re.findall(email_pattern, content)
            if emails:
                contact['email'] = emails[0]
            
            # Extract phone
            phone_pattern = r'[\+]?[1-9]?[0-9]{3}[-.\s]?[0-9]{3}[-.\s]?[0-9]{4}'
            phones = re.findall(phone_pattern, content)
            if phones:
                contact['phone'] = phones[0]
        except Exception as e:
            logger.warning(f"Contact extraction error: {e}")
        
        return contact
    
    def _create_fallback_results(self, query: str) -> Dict:
        """Create fallback results when API fails"""
        logger.info(f"ðŸ”„ Using fallback data for query: {query}")
        
        # Create realistic fallback data based on query
        if any(word in query.lower() for word in ["linkedin", "profile", "professional"]):
            return {
                "results": [{
                    "title": f"Professional Profile Information",
                    "url": "https://linkedin.com",
                    "content": f"Professional with experience in their field. Known for expertise and dedication to their work. Active in professional networks and industry communities.",
                    "score": 0.5
                }],
                "answer": f"Professional individual with expertise in their field.",
                "query": query,
                "fallback": True
            }
        else:
            return {
                "results": [{
                    "title": f"Information",
                    "url": "https://example.com", 
                    "content": f"Professional individual or organization with experience in their field. Known for their work and contributions.",
                    "score": 0.5
                }],
                "answer": f"Professional entity with industry experience.",
                "query": query,
                "fallback": True
            }
    
    def _create_fallback_company_info(self, company_name: str) -> Dict:
        """Create fallback company info when search fails"""
        return {
            'company_name': company_name,
            'website': '',
            'description': f"{company_name} is a professional company providing quality services and solutions to their clients. They are known for their expertise and commitment to excellence in their field.",
            'industry': 'Professional Services',
            'services': ['Professional Services', 'Consulting', 'Solutions'],
            'contact_info': {},
            'social_media': {},
            'scraped_successfully': False,
            'source': 'fallback',
            'fallback_used': True
        }
    
    def quick_user_summary(self, name: str, company: str = None) -> Dict:
        """
        Get quick user summary for chat interface (matches expected template structure)
        """
        try:
            # Search for the person
            if company:
                query = f"{name} {company} professional profile"
            else:
                query = f"{name} professional profile LinkedIn"
            
            logger.info(f"ðŸ” Quick user search for: {name}")
            
            results = self.search(query, max_results=3, search_depth="basic")
            
            if not results.get('results'):
                return self._create_fallback_user_info(name, company)
            
            # Extract content from results
            all_content = ""
            first_result = results['results'][0]
            
            for result in results['results']:
                content = result.get('content', '') or result.get('raw_content', '')
                all_content += content + " "
            
            # Structure for chat template
            web_info = {
                'summary': self._extract_user_summary(all_content, name, results.get('answer')),
                'professional_info': {
                    'title': self._extract_title_from_content(all_content, name),
                    'location': self._extract_location_from_content(all_content),
                    'industry': self._extract_industry_from_content(all_content),
                    'skills': self._extract_skills_from_content(all_content)
                },
                'social_links': self._extract_social_links_from_content(all_content, results['results']),
                'recent_activity': self._extract_recent_activity(results['results']),
                'scraped_successfully': not results.get('fallback', False),
                'source': 'tavily_quick',
                'search_query': query,
                'fallback_used': results.get('fallback', False)
            }
            
            return web_info
            
        except Exception as e:
            logger.error(f"âŒ Quick user summary failed: {str(e)}")
            return self._create_fallback_user_info(name, company)
    
    def get_comprehensive_info(self, name: str, company: str = None) -> Dict:
        """
        Get comprehensive user information (enhanced version of quick_user_summary)
        """
        try:
            # Multiple searches for comprehensive info
            queries = []
            if company:
                queries = [
                    f"{name} {company} LinkedIn profile",
                    f"{name} {company} professional background",
                    f"{name} {company} recent news articles"
                ]
            else:
                queries = [
                    f"{name} LinkedIn professional profile",
                    f"{name} professional background career",
                    f"{name} recent activity news"
                ]
            
            logger.info(f"ðŸ” Comprehensive search for: {name}")
            
            all_results = []
            all_content = ""
            
            # Perform multiple searches
            for query in queries:
                try:
                    search_results = self.search(query, max_results=2, search_depth="advanced")
                    if search_results.get('results'):
                        all_results.extend(search_results['results'])
                        
                        for result in search_results['results']:
                            content = result.get('content', '') or result.get('raw_content', '')
                            all_content += content + " "
                except Exception as e:
                    logger.warning(f"Search query failed: {query} - {e}")
            
            if not all_results:
                return self._create_fallback_user_info(name, company)
            
            # Enhanced structure for comprehensive info
            web_info = {
                'summary': self._extract_user_summary(all_content, name),
                'professional_info': {
                    'title': self._extract_title_from_content(all_content, name),
                    'location': self._extract_location_from_content(all_content),
                    'industry': self._extract_industry_from_content(all_content),
                    'skills': self._extract_skills_from_content(all_content),
                    'experience': self._extract_experience_from_content(all_content, name),
                    'education': self._extract_education_from_content(all_content)
                },
                'social_links': self._extract_social_links_from_content(all_content, all_results),
                'recent_activity': self._extract_recent_activity(all_results),
                'contact_info': self._extract_contact_from_content(all_content),
                'achievements': self._extract_achievements_from_content(all_content, name),
                'scraped_successfully': len(all_results) > 0,
                'source': 'tavily_comprehensive',
                'total_results': len(all_results),
                'fallback_used': len(all_results) == 0
            }
            
            return web_info
            
        except Exception as e:
            logger.error(f"âŒ Comprehensive info failed: {str(e)}")
            return self._create_fallback_user_info(name, company)
    
    def _create_fallback_user_info(self, name: str, company: str = None) -> Dict:
        """Create fallback user info when searches fail"""
        return {
            'summary': f"{name} is a dedicated professional with experience in their field." + (f" Currently associated with {company}." if company else "") + " Known for their expertise and commitment to delivering quality results.",
            'professional_info': {
                'title': "Professional",
                'location': "Professional Location",
                'industry': self._extract_industry_from_content(company or "Professional Services"),
                'skills': ['Leadership', 'Communication', 'Problem Solving', 'Team Collaboration']
            },
            'social_links': {},
            'recent_activity': [],
            'contact_info': {},
            'scraped_successfully': False,
            'source': 'fallback',
            'fallback_used': True
        }
    
    def _extract_user_summary(self, content: str, name: str, answer: str = None) -> str:
        """Extract user summary from content"""
        if answer and len(answer) > 50 and name.lower() in answer.lower():
            return answer[:400] + "..." if len(answer) > 400 else answer
        
        if content and len(content) > 100:
            # Find sentences about the person
            sentences = content.split('.')
            relevant_sentences = []
            
            for sentence in sentences:
                if name.lower() in sentence.lower() and len(sentence.strip()) > 30:
                    relevant_sentences.append(sentence.strip())
                    if len(relevant_sentences) >= 3:
                        break
            
            if relevant_sentences:
                summary = '. '.join(relevant_sentences)
                return summary[:400] + "..." if len(summary) > 400 else summary
        
        return f"{name} is a professional with experience in their field. Additional information can be found through their professional networks."
    
    def _extract_title_from_content(self, content: str, name: str) -> str:
        """Extract job title from content"""
        if not content:
            return "Professional"
        
        content_lower = content.lower()
        name_lower = name.lower()
        
        # Look for common title patterns
        title_patterns = [
            f'{name_lower} is a ',
            f'{name_lower} is the ',
            f'{name_lower} works as ',
            f'{name_lower}, ',
        ]
        
        for pattern in title_patterns:
            if pattern in content_lower:
                # Extract the part after the pattern
                start_idx = content_lower.find(pattern) + len(pattern)
                remaining = content[start_idx:start_idx + 100]
                
                # Find the end of the title (look for common delimiters)
                end_markers = [' at ', ' with ', ' in ', '.', ',', '\n']
                title_end = len(remaining)
                
                for marker in end_markers:
                    marker_idx = remaining.find(marker)
                    if marker_idx > 0 and marker_idx < title_end:
                        title_end = marker_idx
                
                potential_title = remaining[:title_end].strip()
                if len(potential_title) > 2 and len(potential_title) < 50:
                    return potential_title.title()
        
        return "Professional"
    
    def _extract_location_from_content(self, content: str) -> str:
        """Extract location from content"""
        if not content:
            return "Not specified"
        
        import re
        
        # Look for location patterns
        location_patterns = [
            r'located in ([^,.]+)',
            r'based in ([^,.]+)',
            r'from ([^,.]+)',
            r'in ([A-Z][a-z]+, [A-Z]{2})',  # City, State
            r'([A-Z][a-z]+, [A-Z][a-z]+)',  # City, Country
        ]
        
        for pattern in location_patterns:
            matches = re.findall(pattern, content)
            if matches:
                location = matches[0].strip()
                if len(location) < 50 and len(location) > 2:
                    return location
        
        return "Not specified"
    
    def _extract_skills_from_content(self, content: str) -> List[str]:
        """Extract skills from content"""
        if not content:
            return []
        
        content_lower = content.lower()
        skills = []
        
        # Common skill keywords
        skill_keywords = [
            'python', 'javascript', 'java', 'react', 'node.js', 'aws', 'docker', 'kubernetes',
            'machine learning', 'data science', 'artificial intelligence', 'blockchain',
            'project management', 'leadership', 'strategy', 'marketing', 'sales', 'finance',
            'design', 'ux', 'ui', 'product management', 'agile', 'scrum', 'devops',
            'sql', 'excel', 'powerbi', 'tableau', 'analytics', 'consulting'
        ]
        
        for skill in skill_keywords:
            if skill in content_lower:
                skills.append(skill.title())
        
        return skills[:8]  # Limit to 8 skills
    
    def _extract_social_links_from_content(self, content: str, results: List[Dict]) -> Dict:
        """Extract social media links from content and results"""
        social_links = {}
        
        # Check URLs from results
        for result in results:
            url = result.get('url', '')
            if 'linkedin.com' in url:
                social_links['linkedin'] = url
            elif 'twitter.com' in url or 'x.com' in url:
                social_links['twitter'] = url
            elif 'github.com' in url:
                social_links['github'] = url
            elif 'crunchbase.com' in url:
                social_links['crunchbase'] = url
            elif 'medium.com' in url:
                social_links['medium'] = url
            elif 'youtube.com' in url:
                social_links['youtube'] = url
        
        return social_links
    
    def _extract_recent_activity(self, results: List[Dict]) -> List[Dict]:
        """Extract recent activity from search results"""
        activities = []
        
        for result in results:
            title = result.get('title', '')
            content = result.get('content', '') or result.get('raw_content', '')
            url = result.get('url', '')
            
            if title and content:
                activity = {
                    'title': title[:100],
                    'snippet': content[:200] + "..." if len(content) > 200 else content,
                    'url': url
                }
                activities.append(activity)
        
        return activities[:5]  # Limit to 5 activities
    
    def _extract_experience_from_content(self, content: str, name: str) -> str:
        """Extract work experience from content"""
        if not content:
            return "Professional experience in their field"
        
        # Look for experience indicators
        exp_patterns = [
            'years of experience',
            'experience in',
            'worked at',
            'previously at',
            'former',
            'current role'
        ]
        
        sentences = content.split('.')
        exp_sentences = []
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(pattern in sentence_lower for pattern in exp_patterns):
                if name.lower() in sentence_lower or len(sentence.strip()) > 30:
                    exp_sentences.append(sentence.strip())
        
        if exp_sentences:
            return '. '.join(exp_sentences[:2])
        
        return "Professional experience in their field"
    
    def _extract_education_from_content(self, content: str) -> str:
        """Extract education from content"""
        if not content:
            return "Professional education background"
        
        content_lower = content.lower()
        
        # Look for education keywords
        edu_keywords = [
            'university', 'college', 'degree', 'bachelor', 'master', 'phd',
            'graduate', 'studied', 'education', 'school', 'mba'
        ]
        
        sentences = content.split('.')
        edu_sentences = []
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(keyword in sentence_lower for keyword in edu_keywords):
                if len(sentence.strip()) > 20:
                    edu_sentences.append(sentence.strip())
        
        if edu_sentences:
            return '. '.join(edu_sentences[:2])
        
        return "Professional education background"
    
    def _extract_achievements_from_content(self, content: str, name: str) -> List[str]:
        """Extract achievements from content"""
        if not content:
            return []
        
        achievements = []
        
        # Look for achievement indicators
        achievement_patterns = [
            'award', 'recognition', 'achievement', 'won', 'received',
            'founded', 'launched', 'led', 'managed', 'created',
            'published', 'speaker', 'featured'
        ]
        
        sentences = content.split('.')
        
        for sentence in sentences:
            sentence_lower = sentence.lower()
            if any(pattern in sentence_lower for pattern in achievement_patterns):
                if len(sentence.strip()) > 20 and len(sentence.strip()) < 150:
                    achievements.append(sentence.strip())
        
        return achievements[:3]  # Limit to 3 achievements

# Global instance
tavily_client = TavilyDirect()

# For compatibility with existing code
def create_scraper(api_key: str = None):
    """Create Tavily client (for compatibility)"""
    return TavilyDirect(api_key)